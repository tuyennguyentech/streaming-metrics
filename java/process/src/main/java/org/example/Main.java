/*
 * This source file was generated by the Gradle 'init' task
 */
package org.example;

import java.util.List;
import java.util.concurrent.TimeUnit;

import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.java.utils.ParameterTool;
import org.apache.flink.configuration.*;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.core.execution.*;
import org.apache.flink.streaming.api.datastream.AsyncDataStream;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.*;
import org.example.conf.Environment;
import org.example.conf.GestaltCache;
import org.example.operators.MetadataEnrichment;
import org.example.operators.ViewDuplication;
import org.example.sinks.ViewDuplicatedMetrics;
import org.example.sources.RawMetrics;
import org.slf4j.*;

import com.google.protobuf.util.JsonFormat;
import com.twitter.chill.protobuf.ProtobufSerializer;

import io.prometheus.write.v2.Types.Request;

public class Main {
  @SuppressWarnings("unused")
  private static final Logger LOG = LoggerFactory.getLogger(Main.class);

  public String getGreeting() {
    return "Hello World!";
  }

  static Configuration createConf() {
    Configuration conf = new Configuration();
    conf.set(WebOptions.LOG_PATH, "logs/process.log");
    conf.set(
        PipelineOptions.SERIALIZATION_CONFIG,
        List.of(String.format("%s: {type: kryo, kryo-type: registered, class: %s}",
            Request.class.getName(),
            ProtobufSerializer.class.getName())));
    conf.set(RestOptions.PORT, 8083);
    conf.set(CheckpointingOptions.CHECKPOINT_STORAGE, "filesystem");
    conf.set(
        CheckpointingOptions.CHECKPOINTS_DIRECTORY,
        String.format(
            "file://%s",
            System.getProperty("user.dir") + "/checkpoints"));
    return conf;
  }

  static StreamExecutionEnvironment createEnv(String[] args) throws Exception {
    ParameterTool parameters = ParameterTool.fromArgs(args);
    String environmentString = parameters.get("env", "local");
    Environment environment = Environment.fromString(environmentString);
    GestaltCache.setEnvironment(environment);
    Configuration conf = createConf();
    final StreamExecutionEnvironment env;
    switch (environment) {
      case LOCAL:
        env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(
            conf);
        env.setParallelism(1);
        break;
      case DEV:
        env = StreamExecutionEnvironment.getExecutionEnvironment(conf);
        env.setParallelism(2);
        break;
      default:
        throw new Exception("unsupported environment");
    }

    env.enableCheckpointing(1000);
    env.getCheckpointConfig().setCheckpointingConsistencyMode(CheckpointingMode.AT_LEAST_ONCE);
    env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);
    env.getCheckpointConfig()
        .setExternalizedCheckpointRetention(
            ExternalizedCheckpointRetention.RETAIN_ON_CANCELLATION);

    return env;
  }

  public static void main(String[] args) throws Exception {
    final StreamExecutionEnvironment env = createEnv(args);

    KafkaSource<Request> rawMetricsSource = RawMetrics.createSource();
    DataStream<Request> rawMetrics = env
        .fromSource(
            rawMetricsSource,
            WatermarkStrategy.forMonotonousTimestamps(),
            "Raw Metrics Kafka Source")
        .uid("raw-metrics-source");

    DataStream<Request> metadataEnrichedMetrics = AsyncDataStream.unorderedWait(rawMetrics, new MetadataEnrichment(), 100_000, TimeUnit.MILLISECONDS, 100);

    DataStream<Request> viewDuplicatedMetrics = AsyncDataStream.unorderedWait(metadataEnrichedMetrics, new ViewDuplication(), 100_000, TimeUnit.MILLISECONDS, 100);
    // DataStream<Request> tmp = rawMetrics.flatMap((Request value, Collector<Request> out) -> {
      // System.out.println("get data");
    // }).returns(Request.class);
    DataStream<Void> written = AsyncDataStream.unorderedWait(viewDuplicatedMetrics, new ViewDuplicatedMetrics(), 100_000, TimeUnit.MILLISECONDS, 100);
    viewDuplicatedMetrics
        .map(request -> JsonFormat
            .printer()
            .print(request))
        .uid("map-to-json-format")
        .print()
        .uid("print-sink");
    env.execute();
  }
}
